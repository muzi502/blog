---
title: ã€ç¿»è¯‘ã€‘ç»™ VMware vSphere ç”¨æˆ·çš„ Kubernetes ç®€ä»‹
date: 2019-12-10
updated:
categories: æŠ€æœ¯
slug:
tag:
  - kubernetes
  - ç¿»è¯‘
copyright: true
comment: true

---

## æ›´æ–°æ—¥å¿—

- 2019-12-10 åˆæ­¥å¼€å§‹ç¿»è¯‘
- 2019-12-11 è¡¥å……å®Œæ•´
- 2019-12-20 æ ¡ç¨¿

## README

æœ¬æ–‡ç¿»è¯‘è‡ª [Kubernetes Introduction for VMware Users â€“ Part 1: The Theory]( https://blogs.vmware.com/cloudnative/2017/10/25/kubernetes-introduction-vmware-users/ )

By Hany Michaels, Senior Staff Solutions Architect NSBU, VMware

October 25, 2017

å››å…­çº§æœªè¿‡çš„å·¥åœ°è‹±è¯­ç¿»è¯‘ã€å¸Œæœ›å„ä½è¯»è€…é›…æ­£ç¿»è¯‘ä¸å½“çš„éƒ¨åˆ†ğŸ˜

## Kubernetes Introduction for VMware Users â€“ Part 1: The Theory

## ç»™ VMware ç”¨æˆ·çš„ Kubernetes ç®€ä»‹â€”â€”ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è®º

>This is the second part of my â€œKubernetes in the Enterpriseâ€ blog series. As I mentioned in my [last article](http://www.hanymichaels.com/2017/10/04/kubernetes-in-the-enterprise-a-vmware-guide-on-how-to-design-deploy-and-operate-k8saas-with-nsx-t-and-vra/), it is important to get everyone to the same level of understanding about Kubernetes ([K8s](https://kubernetes.io/)) before we can proceed to the design and implementation guides.

è¿™æ˜¯æˆ‘çš„â€œ kubernetes åœ¨ä¼ä¸šä¸­åº”ç”¨â€ åšå®¢ç³»åˆ—çš„ç¬¬äºŒç¯‡æ–‡ç« ã€‚æ­£å¦‚æˆ‘åœ¨ä¸Šä¸€ç¯‡æ–‡ç« æåˆ°çš„ï¼Œåœ¨æˆ‘ä»¬ç»§ç»­è®¾è®¡ä¸å®ç°æŒ‡å—ä¹‹å‰ï¼Œé‡è¦çš„æ˜¯è®©æ¯ä¸ªäººå¯¹ kubernetes çš„ç†è§£éƒ½æ˜¯ç›¸åŒæ°´å¹³çš„ã€‚

> I am not going to take the traditional approach here to explain the Kubernetes architecture and technologies. I will explain everything through comparisons with the [vSphere platform](https://www.vmware.com/products/vsphere.html) that you, as a VMware user, are already familiar with. You can say that this was the approach I would have liked someone to use to introduce K8s to me. The latter could be very confusing and overwhelming to understand at the beginning. Iâ€™d like to add also that I used this approach internally at VMware to introduce Kubernetes to some audiences from different practices, and it has proven to work great and get people up to speed with the core concepts.

åœ¨è¿™é‡Œï¼Œæˆ‘ä¸æ‰“ç®—é‡‡ç”¨ä¼ ç»Ÿçš„æ–¹æ³•æ¥è®²è§£ Kubernetes çš„æ¶æ„å’ŒæŠ€æœ¯ã€‚æˆ‘å°†é€šè¿‡ä¸ [vSphere å¹³å°](https://www.vmware.com/products/vsphere.html) çš„æ¯”è¾ƒæ¥ä¸ºä½ è§£é‡Šæ‰€æœ‰çš„æ¦‚å¿µï¼Œå¯¹ä½ æ¥è®² VMware å·²ç»ç›¸å½“ç†Ÿæ‚‰äº†ã€‚ä½ å¯ä»¥è¯´ï¼Œè¿™å°±æ˜¯æˆ‘å¸Œæœ›åˆ«äººå‘æˆ‘ä»‹ç» k8s çš„æ–¹æ³•ã€‚åè€…åœ¨ä¸€å¼€å§‹å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°éå¸¸å›°æƒ‘å’Œéš¾ä»¥ç†è§£ã€‚æˆ‘è¿˜æƒ³è¡¥å……ä¸€ç‚¹ï¼Œæˆ‘åœ¨ VMware å†…éƒ¨ä¹Ÿæ˜¯ä½¿ç”¨è¿™ç§æ–¹æ³•å‘æ¥è‡ªä¸åŒå®è·µçš„å¬ä¼—ä»¬ä»‹ç» Kubernetesã€‚

> An important note before we kick this off: I am not using this comparison for the sake of it, or to prove any similarities or differences between vSphere and Kubernetes. Both are distributed systems at heart, and they must have similarities like any other similar system out there. What I am trying to achieve here at the end of the day is to introduce an incredible technology like Kubernetes to the broader VMware community.

åœ¨æ­¤ä¹‹å‰æœ‰ä¸€ä¸ªé‡è¦æç¤ºï¼šæˆ‘ä¸æ˜¯ä¸ºäº†ä»‹ç» Kubernetes è€Œä½¿ç”¨è¿™ä¸ªæ¯”è¾ƒï¼Œä¹Ÿä¸æ˜¯ä¸ºäº†è¯æ˜ vSphere å’Œ Kubernetes ä¹‹é—´å­˜åœ¨çš„ä»»ä½•å¼‚åŒä¹‹å¤„ã€‚ä¸¤è€…çš„æ ¸å¿ƒéƒ½æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œå®ƒä»¬è‚¯å®šæœ‰ç€å…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç›¸ä¼¼ä¹‹å¤„ã€‚æœ€åï¼Œæˆ‘æƒ³åœ¨è¿™é‡Œå®ç°çš„ç›®æ ‡æ˜¯å‘æ›´å¹¿æ³›çš„ VMware ç¤¾åŒºä»‹ç»åƒ Kubernetes è¿™æ ·ä¸å¯æ€è®®çš„æŠ€æœ¯ã€‚

![](https://p.k8s.li/kubernetes-architecture-1024x512.png)

> Figure: The Kubernetes overall architecture compared to vSphere

å›¾ç‰‡ï¼šKubernetes å’Œ vSphere æ•´ä½“æ¶æ„å¯¹æ¯”

## A little bit of history

## æ’æ›²

> You should already be familiar with containers before reading this post. I am not going to go through those basics as I am sure there are so many resources out there that talk about this. What I see very often though when I speak with my customers is that they cannot make much sense of why containers have taken our industry by storm and become very popular in record time. To answer this question, and in fact set the context for what is coming, I may have to tell you a little bit about my history as a practical example of how I personally made sense of all the shift that is happening in our industry.

åœ¨é˜…è¯»è¿™ç¯‡æ–‡ç« ä¹‹å‰ï¼Œä½ åº”è¯¥å·²ç»ç†Ÿæ‚‰å®¹å™¨æŠ€æœ¯äº†ã€‚åœ¨æ­¤æˆ‘å°±ä¸å†èµ˜è¿°è¿™äº›åŸºç¡€çŸ¥è¯†äº†ï¼Œå› ä¸ºæˆ‘æ•¢è‚¯å®šç½‘ä¸Šæœ‰å¾ˆå¤šæ•™ç¨‹åœ¨è®²è§£è¿™äº›ã€‚ä¸è¿‡ï¼Œå½“æˆ‘ä¸å®¢æˆ·äº¤è°ˆæ—¶ï¼Œæˆ‘ç»å¸¸æ³¨æ„åˆ°ï¼Œä»–ä»¬æ— æ³•ç†è§£ä¸ºä»€ä¹ˆå®¹å™¨æŠ€æœ¯å¸­æœ‰å²ä»¥æ¥å¸­å·äº†æˆ‘ä»¬çš„è¡Œä¸šå¹¶å˜å¾—éå¸¸æµè¡Œã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸ºæ¥ä¸‹æ¥çš„å†…å®¹åšé“ºå«ï¼Œæˆ‘å¯èƒ½éœ€è¦å‘ä½ ä»‹ç»ä¸€ä¸‹æˆ‘çš„å†å²ï¼Œä»¥æ­¤ä½œä¸ºä¸€ä¸ªå®é™…çš„ä¾‹å­ï¼Œæ¥é˜è¿°æˆ‘ä¸ªäººå¦‚ä½•ç†è§£è¡Œä¸šä¸­æ­£åœ¨å‘ç”Ÿçš„è½¬å˜ã€‚

> I used to be a web developer back in 2003 before I got introduced to the telecom world, and it was my second paying job after being a network engineer/admin. (I know, I was a jack-of-all-trades back then). I used to code in PHP, and Iâ€™ve done all sorts of applications from small internal apps used by my employer, to professional voting apps for TV programs, to telco apps interfacing with VSAT hubs and interacting with satellite systems. Life was great, except for one major hurdle that I am sure every developer can relate to: the dependencies.

æ—©åœ¨ 2003 å¹´è¿›å…¥ç”µä¿¡è¡Œä¸šä¹‹å‰ï¼Œæˆ‘åˆšæˆä¸ºä¸€å web å¼€å‘è€…ï¼Œè¿™æ˜¯æˆ‘åœ¨æˆä¸ºç½‘ç»œå·¥ç¨‹å¸ˆ/ç®¡ç†å‘˜ä¹‹åçš„ç¬¬äºŒä»½æœ‰è–ªå·¥ä½œã€‚ï¼ˆæˆ‘çŸ¥é“ï¼Œé‚£æ—¶çš„æˆ‘æ˜¯ä¸ªä¸‡äº‹é€šï¼‰ã€‚æˆ‘è¿‡å»ä½¿ç”¨ PHP ç¼–å†™ä»£ç ï¼Œæˆ‘å¼€å‘è¿‡å„ç§å„æ ·çš„åº”ç”¨ï¼Œä»æˆ‘çš„é›‡ä¸»ä½¿ç”¨çš„å°å‹å†…éƒ¨åº”ç”¨ï¼Œåˆ°ç”µè§†èŠ‚ç›®çš„ä¸“ä¸šæŠ•ç¥¨åº”ç”¨ï¼Œå†åˆ°ä¸ VSAT é›†çº¿å™¨æ¥å£å’Œå«æ˜Ÿç³»ç»Ÿäº¤äº’çš„ç”µä¿¡åº”ç”¨ç¨‹åºã€‚ç”Ÿæ´»æ˜¯ç¾å¥½çš„ï¼Œé™¤äº†ä¸€ä¸ªä¸»è¦çš„éšœç¢ï¼Œæˆ‘ç›¸ä¿¡æ¯ä¸ªå¼€å‘äººå‘˜æ¶‰åŠåˆ°ï¼šä¾èµ–ã€‚

> I first code my app on my laptop using something like the LAMP stack, and when it works well, I upload the source code to the servers, be it hosted out on the internet (anyone remember RackShack?) or on private servers for our end-customers. As you can imagine, as soon as I do that, my app is broken and it just wonâ€™t work on those servers. The reason, of course, is that the dependencies I use (like Apache, PHP, MySQL, etc.) have different releases than what I used on my laptop. So I have to figure out a way to either upgrade those releases on the remote servers (bad idea) or just re-code what I did on my laptop to match the remote stacks (worse idea). It was a nightmare, and sometimes I hated myself and questioned why Iâ€™m doing this for a living.

é¦–å…ˆæˆ‘åœ¨æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šç”¨ LAMP æŠ€æœ¯æ ˆç¼–å†™æˆ‘çš„åº”ç”¨ç¨‹åºã€‚å½“å°±ç»ªæ—¶ï¼Œæˆ‘ä¸Šä¼ æºä»£ç åˆ°æ‰˜ç®¡åœ¨äº’è”ç½‘çš„æœåŠ¡å™¨ä¸Šï¼ˆè°è¿˜è®°å¾— RackShackï¼Ÿï¼‰æˆ–è€…æˆ‘ä»¬ç»ˆç«¯å®¢æˆ·çš„ç§æœ‰æœåŠ¡å™¨ä¸Šã€‚å¯ä»¥æƒ³è±¡ï¼Œåªè¦æˆ‘è¿™æ ·åšï¼Œæˆ‘çš„åº”ç”¨ç¨‹åºæ˜¯ä¸å®Œæ•´çš„ï¼Œå®ƒå°±æ— æ³•åœ¨è¿™äº›æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚åŸå› å½“ç„¶æ˜¯æˆ‘åœ¨è¿™äº›æœåŠ¡å™¨ä¸Šæ‰€ä½¿ç”¨çš„ä¾èµ–ç¯å¢ƒï¼ˆå¦‚ Apacheã€PHPã€MySQL ç­‰ï¼‰çš„ç‰ˆæœ¬ä¸æˆ‘åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šä½¿ç”¨çš„ç‰ˆæœ¬ä¸åŒã€‚å› æ­¤ï¼Œæˆ‘å¿…é¡»æƒ³åŠæ³•å‡çº§è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„è¿™äº›ç‰ˆæœ¬ï¼ˆé¦Šä¸»æ„ï¼‰ï¼Œæˆ–è€…åªæ˜¯é‡æ–°ç¼–å†™æˆ‘åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šæ‰€åšçš„ä»£ç ï¼Œä»¥åŒ¹é…è¿œç¨‹æŠ€æœ¯æ ˆï¼ˆæ›´ç³Ÿç³•çš„åŠæ³•ï¼‰ã€‚é‚£æ˜¯ä¸€åœºå™©æ¢¦ï¼Œæœ‰æ—¶æˆ‘æ¨è‡ªå·±å¹¶è´¨ç–‘æˆ‘ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšã€‚

> Fast forward 10 years, and along came a company called Docker. I was a VMware consultant in professional services (2013) when I heard about Docker, and let me tell you that I couldnâ€™t make any sense of that technology back in those days. I kept saying things like: â€œWhy would I run containers when I can do that with VMs?â€ â€œWhy would I give up important features like vSphere HA, DRS or vMotion for those weird benefits of booting up a container instantly or skipping the â€œhypervisorâ€ layer?â€ In short, I was looking at this from a pure infrastructure perspective.

å¿«è¿›10å¹´ï¼Œéšä¹‹è€Œæ¥çš„æ˜¯ä¸€å®¶åä¸º Docker çš„å…¬å¸ã€‚å½“æˆ‘å¬è¯´ Docker æ—¶ï¼Œæˆ‘æ˜¯ä¸€å VMware ä¸“ä¸šæœåŠ¡é¡¾é—®ï¼ˆ2013 å¹´ï¼‰ã€‚ä½ æœ‰æ‰€ä¸çŸ¥ï¼Œé‚£æ—¶çš„æˆ‘å¯¹è¿™ç§æŠ€æœ¯ä¸€æ— æ‰€çŸ¥ã€‚æˆ‘æ€»æ˜¯è¿™æ ·è¯´ï¼š"ä¸ºä»€ä¹ˆå½“æˆ‘èƒ½å¤Ÿä½¿ç”¨ VM æ—¶éè¦è¿è¡Œå®¹å™¨å‘¢ï¼Ÿâ€œï¼Œ"æˆ‘ä¸ºä»€ä¹ˆè¦æ”¾å¼ƒé‡è¦ç‰¹æ€§ï¼Œå¦‚ vSphere HAã€DRS æˆ– vMotionï¼Œä¸ºäº†å¿«é€Ÿå¯åŠ¨å®¹å™¨æˆ–è·³è¿‡ `è™šæ‹Ÿæœºç®¡ç†ç¨‹åº`å±‚æ‰€å¸¦æ¥çš„å¥½å¤„ï¼Ÿâ€ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘æ˜¯ä»çº¯ç²¹çš„åŸºç¡€æ¶æ„è§’åº¦æ¥çœ‹å¾…è¿™ä¸ªé—®é¢˜çš„ã€‚

> But then I started looking closer until it just hit me. Everything Docker is all about relates to developers. Only when I started thinking like one did it click. What if I had this technology back in 2003 and packaged my dependencies? My web apps would work no matter what server they run on. Better yet, I donâ€™t have to keep uploading source code or setting up anything special. I can just â€œpackageâ€ my app in an image and tell my customer to download that image and run it. Thatâ€™s a web developerâ€™s dream!

ä½†åæ¥æˆ‘å¼€å§‹åŸºç¡€å®¹å™¨æŠ€æœ¯ï¼Œç›´åˆ°å®ƒåˆšåˆšå†²å‡»åˆ°æˆ‘ã€‚Docker çš„æ‰€æœ‰å†…å®¹éƒ½ä¸å¼€å‘è€…æœ‰å…³ã€‚åªæœ‰å½“æˆ‘å¼€å§‹åƒä¸€ä¸ªäººä¸€æ ·æ€è€ƒæ—¶ï¼Œå®ƒæ‰å‡»ä¸­è¦å®³ã€‚å¦‚æœæˆ‘åœ¨ 2003 å¹´æ‹¥æœ‰æ­¤æŠ€æœ¯å¹¶æ‰“åŒ…äº†ä¾èµ–å…³ç³»ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿæ— è®ºåœ¨ä»€ä¹ˆæœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œæˆ‘çš„ Web åº”ç”¨éƒ½ä¼šå·¥ä½œã€‚æ›´å¥½çš„æ˜¯ï¼Œæˆ‘ä¸éœ€è¦ç»§ç»­ä¸Šä¼ æºä»£ç æˆ–è®¾ç½®ä»»ä½•ç‰¹åˆ«çš„ä¸œè¥¿ã€‚æˆ‘å¯ä»¥åœ¨é•œåƒä¸­"æ‰“åŒ…"æˆ‘çš„åº”ç”¨ç¨‹åºï¼Œå¹¶å‘Šè¯‰æˆ‘çš„å®¢æˆ·ä¸‹è½½è¯¥é•œåƒå¹¶è¿è¡Œå®ƒã€‚è¿™æ˜¯ä¸€ä¸ª web å¼€å‘è€…çš„æ¢¦æƒ³ï¼

> Docker solved a huge issue for interop and packaging, but now what? As an enterprise customer, how can I operate this app at scale? I still want my HA, my DRS, my vMotion and my DR. It solved my developer problems and it created a whole bunch of new ones for my DevOps team. They need a platform to run those containers the same way they used to run VMs.

Docker è§£å†³äº†äº¤äº’å’Œæ‰“åŒ…çš„ä¸€ä¸ªå¤§é—®é¢˜ï¼Œä½†ç°åœ¨æ€ä¹ˆåŠï¼Ÿä½œä¸ºä¼ä¸šå®¢æˆ·ï¼Œæˆ‘å¦‚ä½•å¤§è§„æ¨¡æ“ä½œæ­¤åº”ç”¨ç¨‹åºï¼Ÿæˆ‘ä»ç„¶æƒ³è¦æˆ‘çš„ HAã€æˆ‘çš„ DRSã€æˆ‘çš„ vMotion å’Œæˆ‘çš„ DRã€‚å®ƒè§£å†³äº†æˆ‘çš„å¼€å‘äººå‘˜çš„é—®é¢˜ï¼Œå¹¶ä¸ºæˆ‘çš„ DevOps å›¢é˜Ÿåˆ›å»ºäº†ä¸€å¤§å †æ–°çš„é—®é¢˜ã€‚ä»–ä»¬éœ€è¦ä¸€ä¸ªå¹³å°æ¥è¿è¡Œè¿™äº›å®¹å™¨ï¼Œå°±åƒä»–ä»¬ç”¨æ¥è¿è¡Œ VM ä¸€æ ·ã€‚

> But then along came Google to tell the world that it has been actually running containers for years (and in fact invented them â€“ Google: cgroups), and that the proper way to do that is through a platform they called Kubernetes. They then open sourced it, gave it as gift to the community, and that changed everything again.

ä½†æ˜¯åæ¥ Google å‘Šè¯‰ä¸–ç•Œï¼Œä»–ä»¬å®é™…ä¸Šè¿è¡Œå®¹å™¨å¾ˆå¤šå¹´äº†ï¼ˆå…¶å®æ˜¯ä»–ä»¬å‘æ˜çš„ - Google: cgroupsï¼‰ï¼Œè€Œè¿™æ ·åšçš„æ­£ç¡®æ–¹æ³•æ˜¯é€šè¿‡ä¸€ä¸ªç§°ä¸º Kuberneteså¹³å°ï¼Œç„¶åä»–ä»¬æŠŠ Kubernetes ä½œä¸ºç¤¼ç‰©å¼€æºç»™äº†ç¤¾åŒº ï¼Œè¿™å†æ¬¡æ”¹å˜äº†ä¸€åˆ‡ã€‚

## Understanding Kubernetes by comparing it to vSphere

## é€šè¿‡å’Œ vSphere çš„æ¯”è¾ƒæ¥ç†è§£ Kubernetes

> So what is Kubernetes? Simply put: it is to containers what vSphere was for VMs to make them data center ready. If you used to run VMware Workstation back in the early 2000s, you know that they were not seriously considered for running inside data centers. Kubernetes brings a way to run and operate containers in a production-ready manner. This is why we will start to compare vSphere side-by-side with Kubernetes in order to explain the details of this distributed system and get you up to speed on its features and technologies.

é‚£ä¹ˆä»€ä¹ˆæ˜¯ Kubernetes å‘¢ï¼Ÿç®€å•åœ°è¯´ï¼šKubernetes ä¹‹äºå®¹å™¨å°±åƒ vSphere ä¹‹äº VM ä¸º VM å‡†å¤‡å¥½æ•°æ®ä¸­å¿ƒã€‚å¦‚æœä½ æ›¾ç»åœ¨ 21 ä¸–çºªåˆè¿è¡Œè¿‡ VMware å·¥ä½œç«™ï¼Œä½ çŸ¥é“ä»–ä»¬å¹¶æœªè®¤çœŸè€ƒè™‘åœ¨æ•°æ®ä¸­å¿ƒå†…éƒ¨è¿è¡Œã€‚Kubernetes å¸¦æ¥äº†ä¸€ç§ä»¥ç”Ÿäº§å¯ç”¨çš„æ–¹å¼æ¥è¿è¡Œå’Œæ“ä½œå®¹å™¨çš„æ–¹æ³•ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†å¼€å§‹å°† vSphere ä¸ Kubernetes è¿›è¡Œæ¨ªå‘æ¯”è¾ƒï¼Œä»¥ä¾¿è§£é‡Šæ­¤åˆ†å¸ƒå¼ç³»ç»Ÿçš„ç»†èŠ‚ï¼Œå¹¶è®©ä½ å¿«é€Ÿäº†è§£ Kubernetes çš„ç‰¹æ€§å’ŒæŠ€æœ¯ã€‚

![](https://p.k8s.li/p2p3-1024x375.png)

> Figure: The VM evolution from Workstation to vSphere compared to the current evolution for containers to Kubernetes

å›¾ç‰‡ï¼šä»å®¹å™¨åˆ° Kubernetes çš„ä¸VM ä» Workstation åˆ° vSphere æ¼”è¿›çš„å¯¹æ¯”

## System Overview

## ç³»ç»Ÿæ¦‚è§ˆ

> Just like vSphereâ€™s vCenter and ESXi hosts, Kuberentes has the concept of master and nodes. In this context, the K8s master is equivalent to vCenter in that it is the management plane of the distributed system. It is also the APIsâ€™ entry point where you interact with your workloads management. Similarly, the K8s nodes act as the compute resources like ESXi hosts. This is where you run your actual workloads (in K8sâ€™ case we call them pods). The nodes could be virtual machines or physical servers. In vSphereâ€™s case, of course, the ESXi hosts have to be physical always.

ä¸ vSphere çš„ vCenter å’Œ ESXi ä¸»æœºä¸€æ ·ï¼ŒKuberentes å…·æœ‰ master èŠ‚ç‚¹å’Œ node èŠ‚ç‚¹çš„æ¦‚å¿µã€‚åœ¨æ­¤å¤„ï¼ŒK8s ä¸­çš„ master ç­‰æ•ˆäº vCenterï¼Œå› ä¸ºå®ƒæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„æ§åˆ¶å¹³é¢ã€‚ä¹Ÿæ˜¯ä½ åœ¨é›†ç¾¤ä¸­ç®¡ç†å·¥ä½œè´Ÿè½½çš„ API çš„å…¥å£ã€‚åŒæ ·ï¼ŒK8s ä¸­çš„èŠ‚ç‚¹å……å½“åƒ ESXi ä¸»æœºä¸€æ ·çš„è®¡ç®—èµ„æºã€‚è¿™é‡Œè¿è¡Œç€å®é™…å·¥ä½œè´Ÿè½½ï¼ˆåœ¨ K8s çš„å®ä¾‹ä¸­ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º Podï¼‰ã€‚èŠ‚ç‚¹å¯ä»¥æ˜¯è™šæ‹Ÿæœºæˆ–ç‰©ç†æœåŠ¡å™¨ã€‚å½“ç„¶ï¼Œåœ¨ vSphere ä¸­ï¼ŒESXi ä¸»æœºå¿…é¡»æ˜¯ç‰©ç†æœºã€‚

![](https://p.k8s.li/kubernetes-system-1024x624.png)

You can see also that K8s has a key-value store called â€œetcd.â€ It is similar to vCenter Server DB in that you store the cluster configuration as the desired state you want to adhere to there.

> ä½ è¿˜å¯ä»¥çœ‹åˆ° K8s å…·æœ‰åä¸º"etcd"çš„æ•°æ®åº“æ¥å­˜å‚¨é”®å€¼å¯¹ã€‚å®ƒç±»ä¼¼äº vCenter æœåŠ¡å™¨ DBï¼Œå°†ç¾¤é›†é…ç½®å­˜å‚¨ä¸ºä½ æœŸæœ›çš„æœŸæœ›çŠ¶æ€ã€‚

> On the differences side, K8s master can also run workloads, but vCenter cannot. The latter is just a virtual appliance dedicated to management. In K8s master case, itâ€™s still considered a compute resource, but itâ€™s not a good idea to run enterprise apps on it. Only system related apps would be fine.

ä¸åŒçš„æ˜¯ï¼ŒK8s master èŠ‚ç‚¹ä¹Ÿå¯ä»¥è¿è¡Œå·¥ä½œè´Ÿè½½ï¼Œä½† vCenter ä¸èƒ½è¿è¡Œã€‚åè€…åªæ˜¯ä¸“ç”¨äºç®¡ç†çš„è™šæ‹Ÿæœºã€‚åœ¨ K8s master èŠ‚ç‚¹ï¼Œå®ƒä»ç„¶è¢«è§†ä¸ºè®¡ç®—èµ„æºï¼Œä½†å¹¶ä¸å»ºè®®ç”¨æ¥è¿è¡Œä¼ä¸šåº”ç”¨ã€‚ master èŠ‚ç‚¹åªé€‚åˆè¿è¡Œä¸ kubernetes ç³»ç»Ÿç›¸å…³çš„åº”ç”¨ã€‚

> So, how does this look in the real world? You will mainly use CLI to interact with this system (GUI is also a viable option). In the screenshot below, you can see that I am using a Windows machine to connect to my Kubernetes cluster via command like (I am using cmder in case you are wondering). We see in the screenshot that I have one master and 4 x nodes. They run K8s v1.6.5, and the nodes operating system is Ubuntu 16.04. At the time of this writing, we are mainly living in a Linux world where your master and nodes are always based on Linux distributions.

é‚£ä¹ˆï¼Œåœ¨ç°å®ä¸–ç•Œä¸­æ˜¯æ€æ ·çš„å‘¢ï¼Ÿä½ å°†ä¸»è¦ä½¿ç”¨ CLI ä¸æ­¤ç³»ç»Ÿè¿›è¡Œäº¤äº’ï¼ˆGUI ä¹Ÿæ˜¯ä¸€ä¸ªå¯è¡Œçš„é€‰é¡¹ï¼‰ã€‚åœ¨ä¸‹é¢çš„æˆªå›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘åœ¨ Windows è®¡ç®—æœºä¸Šä½¿ç”¨ç±»ä¼¼çš„å‘½ä»¤ï¼ˆä½¿ç”¨çš„æ˜¯ cmderï¼‰è¿æ¥åˆ°æˆ‘çš„ Kubernetes ç¾¤é›†ã€‚æˆ‘ä»¬åœ¨æˆªå›¾ä¸­çœ‹åˆ°ï¼Œæˆ‘æœ‰ä¸€ä¸ª master èŠ‚ç‚¹å’Œ 4 ä¸ª node èŠ‚ç‚¹ã€‚é›†ç¾¤è¿è¡Œ K8s v1.6.5ï¼ŒèŠ‚ç‚¹æ“ä½œç³»ç»Ÿä¸º Ubuntu 16.04ã€‚åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬ä¸»è¦ç”Ÿæ´»åœ¨ Linux ä¸–ç•Œä¸­ï¼Œmaster èŠ‚ç‚¹å’Œ node èŠ‚ç‚¹å§‹ç»ˆåŸºäº Linux å‘è¡Œç‰ˆã€‚

![](https://p.k8s.li/clidash-1024x563.png)

## Workloads Form-factor

## å·¥ä½œè´Ÿè½½

> In vSphere, a virtual machine is the logical boundary of an operating system. In Kubernetes, pods are the boundaries for containers. Just like an ESXi host that can run multiple VMs, a K8s node can run multiple pods. Each Pod gets a routed IP address just like VMs to communicate with other pods.

åœ¨ vSphere ä¸­ï¼Œè™šæ‹Ÿæœºæ˜¯æ“ä½œç³»ç»Ÿçš„é€»è¾‘è¾¹ç•Œã€‚è€Œåœ¨ Kubernetes ï¼Œpods æ˜¯å®¹å™¨çš„è¾¹ç•Œã€‚ä¸å¯ä»¥è¿è¡Œå¤šä¸ª VM çš„ ESXi ä¸»æœºä¸€æ ·ï¼ŒK8s èŠ‚ç‚¹å¯ä»¥è¿è¡Œå¤šä¸ª Podã€‚æ¯ä¸ª Pod è·å–è·¯ç”± IP åœ°å€ï¼Œå°±åƒ VM ä¸€æ ·ä¸å…¶ä»– Pod è¿›è¡Œé€šä¿¡ã€‚

> In vSphere, applications run inside OS. In Kubernetes, applications run inside containers. A VM can run one single OS, while a Pod can run multiple containers.

åœ¨ vSphere ä¸­ï¼Œåº”ç”¨ç¨‹åºåœ¨æ“ä½œç³»ç»Ÿå†…è¿è¡Œã€‚è€Œåœ¨ Kubernetes ä¸­ï¼Œåº”ç”¨ç¨‹åºåœ¨å®¹å™¨å†…è¿è¡Œã€‚VM å¯ä»¥è¿è¡Œå•ä¸ªæ“ä½œç³»ç»Ÿï¼Œè€Œ Pod å´å¯ä»¥è¿è¡Œå¤šä¸ªå®¹å™¨ã€‚

![](https://p.k8s.li/kubernetes-pods-1024x486.png)

> This is how you can list the pods inside a K8s cluster using the kubectl tool from the CLI. You can check the health of the pods, the age, the IP addresses and the nodes they are currently running inside.

è¿™æ˜¯ä½¿ç”¨ CLI ä¸­çš„ kubectl å·¥å…·åˆ—å‡º K8s ç¾¤é›†ä¸­çš„ Pod çš„æ–¹å¼ã€‚ä½ å¯ä»¥æ£€æŸ¥ Pod çš„è¿è¡ŒçŠ¶å†µã€åˆ›å»ºæ—¶é—´ã€IP åœ°å€ä»¥åŠå®ƒä»¬å½“å‰è¿è¡Œåœ¨å“ªä¸ªèŠ‚ç‚¹ã€‚

![](https://p.k8s.li/cli2-1024x450.png)

## Management

## ç®¡ç†

> So how do we manage our master, nodes and pods? In vSphere, we use the Web Client to manage most (if not all) the components in our virtual infrastructure. This is almost the same with Kubernetes with the use of the Dashboard. It is a nice GUI-based web portal where you can access your browser similarly to  Web Client. Weâ€™ve also seen in the previous sections that you can manage your K8s cluster using the kubectl command from the CLI. Itâ€™s always debatable where you will spend most of your time â€” the CLI or the Dashboard, especially because the latter is becoming more powerful every day (check [this video](https://www.youtube.com/watch?v=3lhf7T9Bp2E) for more details). I personally find the Dashboard very convenient for quickly monitoring the health or showing the details of the various K8s components rather than typing long commands. Itâ€™s a preference, and you will find the balance between them naturally.

é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ç®¡ç†ä¸»æœºã€èŠ‚ç‚¹å’Œ Pod å‘¢ï¼Ÿåœ¨ vSphere ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Web å®¢æˆ·ç«¯æ¥ç®¡ç†è™šæ‹ŸåŒ–åŸºç¡€æ¶æ„ä¸­çš„å¤§å¤šæ•°ï¼ˆå¦‚æœä¸æ˜¯å…¨éƒ¨ï¼‰ç»„ä»¶ã€‚è¿™å’Œåœ¨ Kubernetes ä½¿ç”¨ä»ªè¡¨ç›˜ä¸€æ ·ã€‚è¿™æ˜¯ä¸€ä¸ªé€šè¿‡æµè§ˆå™¨è®¿é—®ã€åŸºäº GUI ã€ç±»ä¼¼äº web å®¢æˆ·ç«¯çš„é—¨æˆ·ç½‘ç«™ã€‚æˆ‘ä»¬åœ¨å‰å‡ èŠ‚ä¸­è¿˜çœ‹åˆ°ï¼Œä½ å¯ä»¥ä½¿ç”¨  kubectl å‘½ä»¤æ¥ç®¡ç† K8s ç¾¤é›†ã€‚ä½ æ€»æ˜¯åœ¨å¤§éƒ¨åˆ†æ—¶é—´é‡ŒèŠ±åœ¨å“ªé‡Œâ€”â€”CLI æˆ–ä»ªè¡¨ç›˜ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºåè€…æ¯å¤©éƒ½åœ¨å˜å¾—æ›´å¼ºå¤§ï¼ˆè¯·æŸ¥çœ‹[æ­¤è§†é¢‘](https://www.youtube.com/watch?v3lhf7T9Bp2E)ï¼Œäº†è§£æ›´å¤šè¯¦æƒ…ï¼‰ã€‚æˆ‘ä¸ªäººè®¤ä¸ºä»ªè¡¨ç›˜éå¸¸æ–¹ä¾¿ï¼Œå¯ä»¥å¿«é€ŸæŸ¥çœ‹è¿è¡ŒçŠ¶å†µæˆ–æ˜¾ç¤ºå„ç§ k8s ç»„ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼Œè€Œä¸æ˜¯è¾“å…¥å¾ˆé•¿çš„å‘½ä»¤ã€‚è¿™æ˜¯ä¸ªäººå–œå¥½ï¼Œä½ ä¼šè‡ªç„¶åœ°åœ¨ä¸¤è€…ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

![](https://p.k8s.li/kubernetes-management-1024x469.png)

## Configurations

## é…ç½®

> One of the very profound concepts in Kubernetes is the desired state of configurations. You declare what you want for almost any Kubernetes component through a YAML file, and you create that using your kubectl (or through dashboard) as your desired state. Kubernetes will always strive from this moment on to keep that as a running state in your environment. For example, if you want to have four replicas of one pod, K8s will keep monitoring those pods. If one dies or the nodes itâ€™s running have issues, it will self-heal and automatically create that pod somewhere else.

Kubernetes ä¸­éå¸¸é‡è¦çš„æ¦‚å¿µä¹‹ä¸€æ˜¯æ‰€æè¿°çš„é…ç½®çŠ¶æ€ã€‚é€šè¿‡ YAML æ–‡ä»¶å‡ ä¹å¯ä»¥å£°æ˜ä»»ä½• Kubernetes ç»„ä»¶æ‰€éœ€çš„èµ„æºï¼Œå¹¶ä½¿ç”¨ kubectl å‘½ä»¤ï¼ˆæˆ–é€šè¿‡ä»ªè¡¨ç›˜ï¼‰åˆ›å»ºè¯¥å¯¹è±¡ä½œä¸ºæ‰€æè¿°çš„çŠ¶æ€ã€‚ä»åˆ›å»ºåå¼€å§‹ï¼Œåœ¨ä½ çš„é›†ç¾¤ä¸­Kubernetes å§‹ç»ˆä¼šåŠªåŠ›å°†ä¿æŒä¸ºæ‰€æè¿°çš„è¿è¡ŒçŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦æœ‰ä¸€ä¸ª pod çš„å››ä¸ªå‰¯æœ¬ï¼ŒK8s å°†ç»§ç»­ç›‘è§†è¿™äº›podã€‚å¦‚æœä¸€ä¸ª pod æŒ‚æ‰æˆ–å®ƒæ­£åœ¨è¿è¡Œçš„èŠ‚ç‚¹æœ‰é—®é¢˜ï¼Œå®ƒå°†è‡ªæˆ‘ä¿®å¤ï¼Œå¹¶è‡ªåŠ¨åœ¨å…¶ä»–èŠ‚ç‚¹åˆ›å»ºè¯¥ pod ã€‚

> Back to our YAML configuration files â€” you can think of them like a .VMX file for a VM, or a .OVF descriptor for a virtual appliance that you want to deploy in vSphere. Those files define the configuration of the workload/component you want to run. Unlike VMX/OVF files that are exclusive to VMs/Appliances, the YAML configuration files are used to define any K8s component like ReplicaSets, Services, Deployments, etc. as we will see in the coming sections.

å›åˆ°æˆ‘ä»¬çš„ YAML é…ç½®æ–‡ä»¶ â€” ä½ å¯ä»¥å°†å®ƒä»¬æƒ³è±¡æˆ ä¸€ä¸ªæè¿° VM æˆ– çš„ .VMX æ–‡ä»¶æˆ–è€…åœ¨ vSphere ä¸­éƒ¨ç½²è™šæ‹Ÿè®¾å¤‡æ‰€éœ€çš„ .OVF æè¿°ç¬¦æ–‡ä»¶ã€‚è¿™äº›æ–‡ä»¶å®šä¹‰è¦è¿è¡Œçš„å·¥ä½œè´Ÿè½½/ç»„ä»¶çš„é…ç½®ã€‚ä¸ VMX/OVF æ–‡ä»¶æ˜¯ VM/è®¾å¤‡ç‹¬æœ‰çš„ä¸åŒçš„æ˜¯ï¼ŒYAML é…ç½®æ–‡ä»¶ç”¨äºå®šä¹‰ä»»ä½• K8s ç»„ä»¶ï¼Œå¦‚ ReplicaSetsã€Servicesã€ Deployments ç­‰ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºã€‚

![](https://p.k8s.li/kubernetes-confiugrations-1024x511.png)

## Virtual Clusters

## è™šæ‹ŸåŒ–é›†ç¾¤

In vSphere, we have physical ESXi hosts grouped logically to form clusters. We can slice those clusters into other virtual clusters called â€œResource Pools.â€ Those resource pools are mostly used for capping resources. In Kubernetes, we have something very similar. We call them â€œnamespaces,â€ and they could also be used to ensure resource quotas as we will see in the next section. They are most commonly used, however, as a means of multi-tenancy across applications (or users if you are using shared K8s clusters). This is also one of the ways  we can perform security segmentation with NSX-T across those namespaces as we will see in future posts.

åœ¨ vSphere ä¸­ï¼Œæˆ‘ä»¬å°† ESXi ç‰©ç†æœºé€»è¾‘åˆ†ç»„ä»¥å½¢æˆç¾¤é›†ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›ç¾¤é›†åˆ†å‰²æˆå…¶ä»–è™šæ‹ŸåŒ–ç¾¤é›†ï¼Œç§°ä¸º"èµ„æºæ± "ã€‚è¿™äº›èµ„æºæ± ä¸»è¦ç”¨äºé™åˆ¶èµ„æºã€‚åœ¨ Kubernetes ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›éå¸¸ç›¸ä¼¼çš„ä¸œè¥¿ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º"å‘½åç©ºé—´"ï¼Œå®ƒä»¬è¿˜å¯ç”¨äºç¡®ä¿èµ„æºé…é¢ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬æœ€å¸¸ç”¨ä½œè·¨åº”ç”¨ç¨‹åºï¼ˆæˆ–è€…ä½¿ç”¨å…±äº« K8s ç¾¤é›†çš„ç”¨æˆ·ï¼‰çš„å¤šç§Ÿæˆ·æ–¹æ³•ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬å¯ä»¥åœ¨è¿™äº›å‘½åç©ºé—´ä½¿ç”¨ NSX-T æ‰§è¡Œå®‰å…¨åˆ†æ®µçš„æ–¹æ³•ä¹‹ä¸€ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥åçš„å¸–å­ä¸­çœ‹åˆ°ã€‚

![](https://p.k8s.li/kubernetes-namespaces-1024x651.png)

## Resource Management

## èµ„æºç®¡ç†

> As I mentioned in the previous section, namespaces in Kubernetes are commonly used as a means of segmentation. The other use case for it is resource allocation, and it is referred to as â€œResource Quotas.â€ As we saw in previous sections, the definition of that is through YAML configuration files where we declare the desirted state. In vSphere, we similarly define this from the Resource Pools settings as you see in the screenshot below.

æ­£å¦‚æˆ‘åœ¨ä¸Šä¸€èŠ‚ä¸­æåˆ°çš„é‚£æ ·ï¼ŒKubernetes ä¸­é€šå¸¸ç”¨å‘½åç©ºé—´æ¥è¿›è¡Œåˆ’åˆ†ã€‚å®ƒçš„å¦ä¸€ä¸ªç”¨é€”æ˜¯èµ„æºåˆ†é…ï¼Œç§°ä¹‹ä¸º"èµ„æºé…é¢"ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢å„èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œå®ƒçš„å®šä¹‰æ˜¯é€šè¿‡ YAML é…ç½®æ–‡ä»¶æ¥å£°æ˜æœŸæ‰€æœ›çš„çŠ¶æ€ã€‚åœ¨ vSphere ä¸­ï¼Œæˆ‘ä»¬åŒæ ·ä»èµ„æºæ± è®¾ç½®ä¸­å®šä¹‰è¿™ä¸€ç‚¹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](https://p.k8s.li/kubernetes-resource-quotas.png)

## Workloads Identification

## æ ‡è®°å·¥ä½œè´Ÿè½½

> This is fairly easy and almost identical between vSphere and Kubernetes. In the former, we use the concepts of tags to identify or group similar workloads, while in the latter we use the term â€œlabelsâ€ to do this. In Kubernetesâ€™ case, this is mandatory where we use things like â€œselectorsâ€ to identify our containers and apply the different configurations for them.

æ ‡è®°å·¥ä½œè´Ÿè½½ç›¸å½“å®¹æ˜“ä¸” vSphere å’Œ Kubernetes å‡ ä¹ä¸€æ ·ã€‚åœ¨ vSphere ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ tags çš„æ¦‚å¿µæ¥è¯†åˆ«æˆ–åˆ†ç»„ç›¸ä¼¼çš„å·¥ä½œè´Ÿè½½ï¼Œè€Œåœ¨ Kubernetes ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ¯è¯­"labels"æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚åœ¨ Kubernetes çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨"é€‰æ‹©å™¨"ä¹‹ç±»æ¥è¯†åˆ«æˆ‘ä»¬çš„å®¹å™¨å¹¶ä¸ºå…¶åº”ç”¨ä¸åŒçš„é…ç½®ã€‚

![](https://p.k8s.li/kubernetes-labels-1024x331.png)

## Redundancy

## å†—ä½™

> Now to the real fun. If you were/are a big fan of vSphere FT like me, you will love this feature in Kubernetes despite some differences in the two technologies. In vSphere, this is a VM with a running instance and a shadow one in a lock-step. We record the instructions from the running instance and replay it in the shadow VM. If the running instance goes down, the shadow VM kicks in immediately. vSphere then tries to find another ESXi host to bring another shadow instance to maintain the same redundancy. In Kubernetes, we have something very similar here. The ReplicaSets are a number you specify to run multiple instances of a pod. If one pod goes down, the other instances are available to serve the traffic. In the same time, K8s will try to bring up a substitute for that pod on any available node to maintain the desired state of the configuration. The major difference, as you may have already noticed, is that in the case of K8s, the pod instances are always live and service traffic. They are not shadowed workloads.

ç²¾å½©ç°åœ¨å¼€å§‹ã€‚å¦‚æœä½ å’Œæˆ‘ä¸€æ ·çƒ­è¡·äº vSphere FTï¼Œåœ¨ Kubernetes ä¸­ä½ ä¼šå–œæ¬¢è¿™ä¸ªç‰¹æ€§ï¼Œå°½ç®¡è¿™ä¸¤ä¸ªæŠ€æœ¯æœ‰ä¸€äº›å·®å¼‚ã€‚åœ¨ vSphere ä¸­ï¼ŒvSphere FT æ˜¯ä¸€ä¸ªå…·æœ‰æ­£åœ¨è¿è¡Œçš„ VM å½±å­å®ä¾‹ ã€‚æˆ‘ä»¬è®°å½•æ­£åœ¨è¿è¡Œçš„å®ä¾‹ä¸­çš„æŒ‡ä»¤ï¼Œå¹¶åœ¨å·å½± VM ä¸­é‡æ–°æ‰§è¡Œå®ƒã€‚å¦‚æœæ­£åœ¨è¿è¡Œçš„å®ä¾‹å‡ºç°æ•…éšœï¼Œåˆ™å·å½± VM ä¼šç«‹å³å¯åŠ¨ã€‚ç„¶åï¼ŒvSphere ä¼šå°è¯•å¯»æ‰¾å¦ä¸€å° ESXi ä¸»æœºï¼Œæ¥å¯¼å…¥å¦ä¸€ä¸ªå·å½± VM å®ä¾‹ä»¥ç»´æŠ¤ç›¸åŒçš„å†—ä½™ã€‚åœ¨ Kubernetes ä¸­ä¹Ÿæœ‰ç±»ä¼¼çš„ç‰¹æ€§ã€‚å‰¯æœ¬é›†ç”¨æ¥è¿è¡ŒæŒ‡å®šçš„æ•°é‡ pod çš„å®ä¾‹ã€‚å¦‚æœä¸€ä¸ª pod å‡ºç°æ•…éšœï¼Œåˆ™å…¶ä»– pod å®ä¾‹å¯ç»§ç»­å¯¹å¤–æµé‡ææœåŠ¡ã€‚ä¸æ­¤åŒæ—¶ï¼ŒK8s å°†å°è¯•å°†è¯¥ pod è°ƒåº¦åˆ°ä»»ä½•å¯ç”¨èŠ‚ç‚¹ä¸Šï¼Œä»¥ç»´æŒé…ç½®æ–‡ä»¶æ‰€æè¿°çš„çŠ¶æ€ã€‚ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œä¸»è¦åŒºåˆ«æ˜¯ï¼Œåœ¨ K8s ä¸­ï¼Œpod å®ä¾‹å§‹ç»ˆæ˜¯å­˜æ´»çŠ¶æ€å¹¶å¯¹å¤–æä¾›æœåŠ¡ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯éšè—çš„å·¥ä½œè´Ÿè½½ã€‚

![](https://p.k8s.li/kuberentes-replicasets-1024x546.png)

## Load Balancing

## è´Ÿè½½å‡è¡¡

> While this might not be a built-in feature in vSphere, it is still a common thing to run load-balancers on that platform. In the vSphere world, we have either virtual or physical load-balancers to distribute the network traffic across multiple VMs. This could be running in many different configuration modes, but letâ€™s assume here that we are referring to the one-armed configuration. In this case, you are load-balancing your network traffic east-west to your VMs.

è™½ç„¶è´Ÿè½½å‡è¡¡å¯èƒ½ä¸æ˜¯ vSphere ä¸­çš„å†…ç½®åŠŸèƒ½ï¼Œä½†åœ¨è¯¥å¹³å°ä¸Šè¿è¡Œè´Ÿè½½å‡è¡¡å™¨å¾ˆæ™®éã€‚åœ¨ vSphere ä¸­ï¼Œæˆ‘ä»¬æœ‰è™šæ‹Ÿæˆ–ç‰©ç†è´Ÿè½½å‡è¡¡å™¨ï¼Œç”¨äºåœ¨å¤šä¸ª VM ä¹‹é—´å‡è¡¡ç½‘ç»œæµé‡ã€‚è¿™å¯èƒ½åœ¨è®¸å¤šä¸åŒçš„é…ç½®æ¨¡å¼ä¸‹è¿è¡Œï¼Œä½†è®©æˆ‘ä»¬åœ¨è¿™é‡Œå‡è®¾æˆ‘ä»¬æŒ‡çš„æ˜¯å•å…µé…ç½®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å°†ä»ä¸œåˆ°è¥¿åˆ° VM çš„ç½‘ç»œæµé‡è¿›è¡Œè´Ÿè½½å¹³è¡¡ã€‚

> Similarly in Kubernetes, we have the concepts of â€œServices.â€ A K8s Service could also be used in different configuration modes, but letâ€™s pick the â€œClusterIPâ€ configuration here to compare to the one-armed LB. In this case, our K8s Service will have a virtual IP (VIP) that is always static and does not change. This VIP will distribute the traffic across multiple pods. This is especially important in the Kubernetes world were pods are ephemeral by nature and where you lose the pod IP address the moment it dies or gets deleted. So you have to always be able to maintain a static VIP.

åŒæ ·ï¼Œåœ¨ Kubernetes ä¸­ï¼Œæˆ‘ä»¬æœ‰"æœåŠ¡"çš„æ¦‚å¿µã€‚K8s æœåŠ¡ï¼Œä½†è®©æˆ‘ä»¬åœ¨è¿™é‡Œé€‰æ‹©"ClusterIP"é…ç½®ï¼Œä»¥ä¾¿ä¸å•è‡‚ LB è¿›è¡Œæ¯”è¾ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ K8s æœåŠ¡å°†æœ‰ä¸€ä¸ªå§‹ç»ˆé™æ€ä¸”ä¸ä¼šæ›´æ”¹çš„è™šæ‹Ÿ IP ï¼ˆVIPï¼‰ã€‚æ­¤ VIP å°†åœ¨å¤šä¸ª Pod ä¸Šåˆ†é…æµé‡ã€‚è¿™ä¸€ç‚¹åœ¨ Kubernetes ä¸–ç•Œå°¤å…¶é‡è¦ï¼Œå› ä¸ºåŠèˆ±æœ¬è´¨ä¸Šæ˜¯çŸ­æš‚çš„ï¼Œå½“ä½ æ­»æˆ–è¢«åˆ é™¤çš„æ—¶å€™ï¼Œä½ å¤±å»äº† pod IPåœ°å€ã€‚å› æ­¤ï¼Œä½ å¿…é¡»å§‹ç»ˆèƒ½å¤Ÿç»´æŠ¤é™æ€ VIPã€‚

> As I mentioned, the Services have many other configurations like â€œNodePort,â€ where you basically assign a port on the node level and then do a port-address-translation down to the pods. There is also the â€œLoadBalancer,â€ where you spin up an LB instance from a 3rd-party or a cloud provider.

æ­£å¦‚æˆ‘æåˆ°çš„ï¼Œ Services å…·æœ‰è®¸å¤šå…¶ä»–é…ç½®ï¼Œå¦‚"NodePort"ï¼Œå…¶ä¸­ä½ åŸºæœ¬ä¸Šåœ¨èŠ‚ç‚¹çº§åˆ«ä¸Šåˆ†é…ä¸€ä¸ªç«¯å£ï¼Œç„¶åå‘ä¸‹åˆ° Pod æ‰§è¡Œç«¯å£åœ°å€è½¬æ¢ã€‚è¿˜æœ‰"LoadBalancer"ï¼Œä»ç¬¬ä¸‰æ–¹æˆ–äº‘æä¾›å•†å¯åŠ¨ LB å®ä¾‹ã€‚

![](https://p.k8s.li/kubernetes-services-1024x389.png)

> There is another very important load-balancing mechanism in Kuberentes called â€œIngress Controller.â€ You can think of this like an in-line application load-balancer. The core concept behind this is that an ingress-controller (in a form of pod) would be spun up with an externally visible IP address, and that IP could have something like a wild card DNS record. When traffic hits an ingress-controller using the external IP, it will inspect the headers and determine through a set of rules you pre-define to which pod that hostname should belong. Example: _sphinx-v1_.esxcloud.net will be directed to the service â€œsphinx-svc-1â€, while the _sphinx-v2_.esxcloud.net will be directed to the service â€œsphinx-svc2â€ and so on and so forth.

åœ¨ Kuberentes ä¸­è¿˜æœ‰å¦ä¸€ä¸ªéå¸¸é‡è¦çš„è´Ÿè½½å‡è¡¡æœºåˆ¶ï¼Œç§°ä¹‹ä¸º" Ingress æ§åˆ¶å™¨"ã€‚ä½ å¯ä»¥æŠŠå®ƒå½“ä½œä¸€ä¸ªåœ¨çº¿åº”ç”¨è´Ÿè½½å‡è¡¡å™¨ã€‚The core concept behind this is that an ingress-controller (in a form of pod) would be spun up with an externally visible IP address, and that IP could have something like a wild card DNS record. å½“æµé‡ä½¿ç”¨ external IP è¿›å…¥ ingress æ§åˆ¶å™¨æ—¶ï¼Œå®ƒå°†æ£€æŸ¥è¯·æ±‚å¤´éƒ¨å¹¶é€šè¿‡ä¸»æœºåæ¥åˆ¤æ–­æµé‡åº”å±äºå“ªä¸ªä¸€ç»„ pod ã€‚ç¤ºä¾‹ï¼š_sphinx-v1_.esxcloud.net å°†å®šå‘åˆ°æœåŠ¡"sphinx-svc-1"ï¼Œè€Œ _sphinx-v2_.esxcloud.net å°†é‡å®šå‘åˆ°æœåŠ¡"sphinx-svc2"ç­‰ã€‚

![](https://p.k8s.li/kubernetes-ingress-1024x532.png)

## Storage & Networking

## å­˜å‚¨å’Œç½‘ç»œ

> Storage and networking are rich topics when it comes to Kubernetes. It is almost impossible to talk briefly about these two topics in an introduction blog post, but you can be sure that I will be blogging in details soon about the different concepts and options for each subject. For now, letâ€™s quickly examine how the networking stack works in Kubernetes since we will have a dependency on it in a later section.

å½“è°ˆåˆ°åˆ° Kubernetes æ—¶ï¼Œå­˜å‚¨å’Œç½‘ç»œæ˜¯é‡ç‚¹å…³æ³¨çš„è¯é¢˜ã€‚åœ¨ä»‹ç»æ€§çš„åšå®¢æ–‡ç« ä¸­ç®€è¦åœ°è°ˆè®ºè¿™ä¸¤ä¸ªä¸»é¢˜å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œä½†ä½ å¯ä»¥è‚¯å®šï¼Œæˆ‘å°†å¾ˆå¿«åœ¨åšå®¢ä¸Šè¯¦ç»†ä»‹ç»æ¯ä¸ªä¸»é¢˜çš„ä¸åŒæ¦‚å¿µå’Œç‰¹æ€§ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿç ”ç©¶ä¸€ä¸‹ç½‘ç»œå †æ ˆåœ¨ Kubernetes ä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå› ä¸ºæˆ‘ä»¬å°†åœ¨åé¢çš„ä¸€èŠ‚ä¸­ä¾èµ–äºå®ƒã€‚

> Kubernetes has different networking â€œpluginsâ€ that you can use to set up your nodes and pods network. One of the common plugins is â€œkubenet,â€ which is currently used on mega-clouds like Google Cloud Provider (GCP) and Amazon Web Services. I am going to talk briefly here about the GCP implementation and then show you a practical example later to examine this yourself on Google Container Engine (GKE).

Kubernetes å…·æœ‰ä¸åŒçš„ç½‘ç»œ"æ’ä»¶"ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™äº›æ’ä»¶æ¥è®¾ç½®èŠ‚ç‚¹å’Œ Pod ç½‘ç»œã€‚å¸¸è§çš„æ’ä»¶ä¹‹ä¸€æ˜¯"kubenet"ï¼Œå®ƒç›®å‰ç”¨äºåƒè°·æ­Œäº‘æä¾›å•†ï¼ˆGCPï¼‰å’Œäºšé©¬é€Šç½‘ç»œæœåŠ¡è¿™æ ·çš„äº‘æœåŠ¡å•†å·¨å¤´ã€‚æˆ‘å°†åœ¨è¿™é‡Œç®€è¦åœ°è°ˆè°ˆ GCP çš„å®ç°ï¼Œç„¶åå‘ä½ å±•ç¤ºä¸€ä¸ªå¯ä»¥åœ¨è°·æ­Œå®¹å™¨å¼•æ“ï¼ˆGKEï¼‰ä¸Šäº²è‡ªç ”ç©¶çš„å®ä¾‹ã€‚

![](https://p.k8s.li/gke-kubernetes-networking-1024x747.png)

> This might be a bit too much to take in from a first glance, but hopefully you will be able to make sense of all that by the end of this blog post. First, we see that we have two Kubernetes nodes here: node 1 and node (m). Each node has an eth0 interface like any Linux machine, and that interface has an IP address to the external worldâ€”in our case here on subnet 10.140.0.0/24. The upstream L3 device is acting as our default gateway to route our traffic. This could be a L3 switch in your data center or a VPC router in a public cloud like GCP as we will see later. So far so good?

ä¹ä¸€çœ‹ï¼Œè¿™ä¹Ÿè®¸æœ‰ç‚¹å¤ªè¿‡åˆ†äº†ï¼Œä½†å¸Œæœ›ä½ èƒ½åœ¨åšå®¢æ–‡ç« çš„ç»“å°¾ç†è§£è¿™ä¸€åˆ‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬è¿™é‡Œæœ‰ä¸¤ä¸ª Kubernetes èŠ‚ç‚¹ï¼šèŠ‚ç‚¹ 1 å’ŒèŠ‚ç‚¹ ï¼ˆmï¼‰ã€‚æ¯ä¸ªèŠ‚ç‚¹éƒ½åƒå…¶ä»– Linux æœºå™¨ä¸€æ ·å…·æœ‰ eth0 æ¥å£ï¼Œå¹¶ä¸”è¯¥æ¥å£å…·æœ‰ 10.140.0.0/24 å­ç½‘å†…çš„ IP åœ°å€ã€‚ä¸Šæ¸¸ L3 è®¾å¤‡å……å½“æˆ‘ä»¬çš„é»˜è®¤ç½‘å…³ï¼Œç”¨äºè·¯ç”±æˆ‘ä»¬çš„æµé‡ã€‚è¿™å¯èƒ½æ˜¯æ•°æ®ä¸­å¿ƒä¸­çš„ L3 äº¤æ¢æœºï¼Œä¹Ÿå¯ä»¥æ˜¯å…¬å…±äº‘ä¸­çš„ VPC è·¯ç”±å™¨ï¼ˆå¦‚ GCPï¼‰ï¼Œæˆ‘ä»¬ç¨åå°†å¯¹æ­¤è¿›è¡Œä»‹ç»ã€‚ç›®å‰ä¸ºæ­¢ï¼Œè¿˜å¯ä»¥ç†è§£å—ï¼Ÿ

> Next, we see inside the node that we have a **cbr0** bridge interface. That interface has the default gateway of an IP subnetâ€”10.40.1.0/24 in case of node 1. This subnet gets assigned by Kubernetes to each node. The latter usually get a /24 subnet, but you can control that in case of NSX-T as we will see in future posts. For now, this subnet is the one that we will allocate the podâ€™s IP addresses from. Any pod inside node 1 will get an IP address from this subnet rageâ€”in our case here, Pod 1 has an IP address of 10.40.1.10. Notice however that this pod has two nested containers within. Remember, a pod can run one or more containers that are tightly coupled together in terms of functionality. This is what we see here. Container 1 is listening to port 80, while container 2 is listening to port 90. Both containers share the same IP address of 10.40.1.10. but they do not own that networking namespace. Alright, so who owns this networking stack then? Itâ€™s actually a special container that we call the â€œPause Container.â€ You see it in the diagram as the interface of that pod to the outer world. It owns this networking stack, including the IP address 10.40.1.10 and, of course, it forwards the traffic to container 1 using port 80 and traffic to container 2 using port 90.

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨èŠ‚ç‚¹å†…ï¼Œæœ‰ä¸€ä¸ª `cbr0` ç½‘æ¡¥æ¥å£ã€‚è¯¥æ¥å£å…·æœ‰ IP å­ç½‘çš„é»˜è®¤ç½‘å…³ â€”åœ¨èŠ‚ç‚¹ 1 çš„æƒ…å†µä¸‹â€”ï¼š10.40.1.0/24ã€‚æ­¤å­ç½‘ç”± Kubernetes åˆ†é…ç»™æ¯ä¸ªèŠ‚ç‚¹ã€‚åè€…é€šå¸¸å¾—åˆ°ä¸€ä¸ª 24 ä½çš„å­ç½‘ï¼Œä½†ä½ å¯ä»¥æ§åˆ¶åœ¨ NSX-T çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥åçš„å¸–å­ä¸­çœ‹åˆ°ã€‚ç°åœ¨ï¼Œæ­¤å­ç½‘æ˜¯æˆ‘ä»¬å°†ä»ä¸­åˆ†é… Pod çš„ IP åœ°å€çš„å­ç½‘ã€‚èŠ‚ç‚¹ 1 å†…çš„ä»»ä½• pod éƒ½å°†ä»æ­¤å­ç½‘ä¸­è·å¾— IP åœ°å€ï¼Œ åœ¨æˆ‘ä»¬çš„å®ä¾‹ä¸­ï¼ŒPod 1 çš„ IP åœ°å€ä¸º 10.40.1.10ã€‚ä½†è¯·æ³¨æ„ï¼Œæ­¤ pod å†…æœ‰ä¸¤ä¸ªå®¹å™¨ã€‚è¯·è®°ä½ï¼Œpod å¯ä»¥è¿è¡Œä¸€ä¸ªæˆ–å¤šä¸ªåœ¨åŠŸèƒ½æ–¹é¢ç´§å¯†ç›¸å…³çš„å®¹å™¨ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„ã€‚å®¹å™¨ 1 ç›‘å¬ç«¯å£ 80ï¼Œè€Œå®¹å™¨ 2 ç›‘å¬ç«¯å£ 90ã€‚ä¸¤ä¸ªå®¹å™¨å…±äº«åŒä¸€ä¸ª IP åœ°å€ 10.40.1.10ã€‚ä½†ä»–ä»¬å¹¶ä¸ç‹¬è‡ªæ‹¥æœ‰ç½‘ç»œå‘½åç©ºé—´ã€‚å¥½å§ï¼Œé‚£ä¹ˆè°æ‹¥æœ‰è¿™ä¸ªç½‘ç»œå †æ ˆå‘¢ï¼Ÿå®ƒå®é™…ä¸Šæ˜¯ä¸€ä¸ªç‰¹æ®Šå®¹å™¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º" puse å®¹å™¨"ã€‚åœ¨å›¾ä¸­ï¼Œä½ å°†å…¶è§†ä¸ºè¯¥ pod ä¸å¤–éƒ¨ä¸–ç•Œçš„æ¥å£ã€‚å®ƒæ‹¥æœ‰æ­¤ç½‘ç»œå †æ ˆï¼ŒåŒ…æ‹¬ IP åœ°å€ 10.40.1.10ï¼Œå½“ç„¶ï¼Œå®ƒä½¿ç”¨ç«¯å£ 80 å°†æµé‡è½¬å‘åˆ°å®¹å™¨ 1ï¼Œä½¿ç”¨ç«¯å£ 90 å°†æµé‡è½¬å‘åˆ°å®¹å™¨ 2ã€‚

> Now you should be asking, how is traffic forwarded to the external world? You see that we have a standard Linux IP forwarding enabled here to forward the traffic from cbr0 to eth0. This is great, but then how does the L3 device know how to forward this to the destination? We do not have dynamic routing here to advertise this network in this particular example. And so, this is why we need to have some kind of â€œstatic routeâ€ on that L3 device to know that in order to reach subnet 10.40.1.0/24, your entry point is the external IP of node 1 (10.140.0.11), and in order to reach subnet 10.40.2.0/24, your next hope is node (m) with the IP address 10.140.0.12.

ç°åœ¨ä½ å¯èƒ½ä¼šé—®ï¼Œæµé‡æ˜¯å¦‚ä½•è½¬å‘åˆ°é›†ç¾¤å¤–çš„å‘¢ï¼Ÿä½ çœ‹ï¼Œæˆ‘ä»¬åœ¨æ­¤å¯ç”¨äº†æ ‡å‡†çš„ Linux IP è½¬å‘åŠŸèƒ½ï¼Œå°†æµé‡ä» cbr0 è½¬å‘åˆ° eth0ã€‚è¿™æ˜¯éå¸¸æ£’çš„ï¼Œä½†ç„¶å L3 è®¾å¤‡å¦‚ä½•çŸ¥é“å¦‚ä½•è½¬å‘åˆ°ç›®çš„åœ°ï¼Ÿåœ¨æ­¤ç‰¹å®šç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰åŠ¨æ€è·¯ç”±æ¥é€šå‘Šæ­¤ç½‘ç»œã€‚å› æ­¤ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æœ‰æŸç§"é™æ€è·¯ç”±"åœ¨ L3 è®¾å¤‡ä¸ŠçŸ¥é“ï¼Œä¸ºäº†è¾¾åˆ°å­ç½‘10.40.1.0/24ï¼Œä½ çš„å…¥å£ç‚¹æ˜¯èŠ‚ç‚¹1ï¼ˆ10.140.0.11ï¼‰çš„å¤–éƒ¨IPï¼Œå¹¶ä¸ºäº†è¾¾åˆ°å­ç½‘10.40.2.0/24ï¼Œä½ çš„ä¸‹ä¸€è·³ï¼ˆmï¼‰ ä¸ IP åœ°å€ 10.140.0.12ã€‚

> This is great, but you must be thinking that itâ€™s a very unpractical way to manage your networks. This would be an absolute nightmare for network administrators to maintain all those routes as you scale with your cluster. And youâ€™re rightâ€”this is why we need some kind of solution like the CNI (container network plugin) in Kubernetes to use a networking mechanism to manage this for you. NSX-T is one of those solutions with a powerful design for both the networking and security stacks.

è¿™å¤ªå¥½äº†ï¼Œä½†ä½ ä¸€å®šè®¤ä¸ºï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ä¸å®é™…çš„æ–¹å¼æ¥ç®¡ç†ä½ çš„ç½‘ç»œã€‚å¯¹äºç½‘ç»œç®¡ç†å‘˜æ¥è¯´ï¼Œåœ¨ä¸ç¾¤é›†è¿›è¡Œæ‰©å±•æ—¶ç»´æŠ¤æ‰€æœ‰è¿™äº›è·¯ç”±ç»å¯¹æ˜¯ä¸€åœºå™©æ¢¦ã€‚ä½ è¯´å¾—å¯¹ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æŸç§è§£å†³æ–¹æ¡ˆï¼Œå¦‚ Kubernetes ä¸­çš„CNIï¼ˆå®¹å™¨ç½‘ç»œæ’ä»¶ï¼‰ï¼Œä»¥ä½¿ç”¨ç½‘ç»œæœºåˆ¶ä¸ºä½ ç®¡ç†ã€‚NSX-T æ˜¯è¿™äº›è§£å†³æ–¹æ¡ˆä¹‹ä¸€ï¼Œå…·æœ‰å¼ºå¤§çš„ç½‘ç»œå’Œå®‰å…¨å †æ ˆè®¾è®¡ã€‚

> Remember, we are examining here the kubernetes plugin, not CNI. The former is what GKE uses, and the way they do this is quite fascinating as itâ€™s completely programmable and automated on their cloud. Those subnet allocations and associated routes are taken care of by GCP for you, as we will see in the next part.

è¯·è®°ä½ï¼Œæˆ‘ä»¬æ­£åœ¨æµ‹è¯• kubernetes æ’ä»¶è€Œä¸æ˜¯CNIã€‚å‰è€…æ˜¯ GKE ä½¿ç”¨çš„ï¼Œä»–ä»¬è¿™æ ·åšçš„æ–¹å¼æ˜¯ç›¸å½“æ£’çš„ï¼Œå› ä¸ºå®ƒæ˜¯å®Œå…¨å¯ç¼–ç¨‹å’Œè‡ªåŠ¨åŒ–çš„äº‘ã€‚è¿™äº›å­ç½‘åˆ†é…å’Œç›¸å…³è·¯ç”±ç”± GCP ä¸ºä½ è´Ÿè´£ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­çœ‹åˆ°ã€‚
